'''
Author: Ryan Dalrymple
Date:   5/20/14
Summary: This file will introduce other metrics that are not currently present within the cuML library. With this,
            I will be able to evaluate how good the standard cuML models are in a way that is not currently possible
            with the given API.
'''

import cupy as cp

def findF1(confusionMatrix):
    """Calculates the F1 score for a set of a predictions given a confusionMatrix

    Args:
        confusionMatrix (cuPy array): The matrix generated by the cuml.metrics function confusion_matrix

    Returns:
        avgF1 (float): The average F1 score associated with the confusionMatrix
    """
    # Calculate precision and recall for each class
    precision   = cp.diag(confusionMatrix) / confusionMatrix.sum(axis=0)
    recall      = cp.diag(confusionMatrix) / confusionMatrix.sum(axis=1)

    # Compute F1 score for each class
    f1Scores = 2 * (precision * recall) / (precision + recall)

    # Average F1 scores across all classes
    avgF1 = cp.nanmean(f1Scores)  # Use cp.nanmean to handle potential division by zero
    avgF1 = avgF1.item()
    return avgF1